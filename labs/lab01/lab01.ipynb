{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5fc5f6",
   "metadata": {},
   "source": [
    "\n",
    "# AI Detective Lab 02 – Training the Tools Before the Next Breakthrough\n",
    "\n",
    "In the last homework, you made some progress on the case. But the chief is getting impatient.\n",
    "\n",
    "> *\"I don't want vibes, I want **models**,\"* she says.  \n",
    "> *\"Before you tackle the full estate data again, go back to the lab and get your tools in order.\"*\n",
    "\n",
    "This lab is all about **syntax and mechanics**. The ideas of decision trees, probabilities, and the Viterbi algorithm will be covered in lecture. Here, you will:\n",
    "\n",
    "- Practice turning narrative **statements** into numeric feature vectors.\n",
    "- Train and inspect a simple **decision tree classifier** in scikit-learn.\n",
    "- See why long chains of probabilities should be computed in **log space**.\n",
    "- Implement and run a small **Viterbi algorithm** on a tiny map of rooms.\n",
    "- Combine scores from different models into a single probability distribution.\n",
    "\n",
    "This lab is meant to be *gentle*. It is **not** trying to trick you. If you can get through this notebook, you will have most of the **syntax** you need for Homework 02.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef6e06",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Lab Setup\n",
    "\n",
    "Run the cell below to import the libraries we will use.\n",
    "\n",
    "If you see any errors, let your instructor or TA know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71860c6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. From Narrative Statements to Features\n",
    "\n",
    "You return to the evidence board. There are stacks of **interview notes**—descriptions of where different people claimed to be, and what they were doing.\n",
    "\n",
    "Unfortunately, computers don't understand sentences like:\n",
    "\n",
    "> \"I was reading in the Study, and then I went to make tea in the Kitchen.\"\n",
    "\n",
    "We need to turn these into **numeric clues**: 0/1 features that say things like:\n",
    "\n",
    "- `in_kitchen = 1` if the Kitchen is mentioned, else 0.\n",
    "- `in_study = 1` if the Study is mentioned, else 0.\n",
    "- `mentions_knife = 1` if a knife is mentioned, else 0.\n",
    "\n",
    "Below is a tiny dataset of *toy* statements to get you started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_statements = [\n",
    "    {\n",
    "        \"suspect\": \"Alex\",\n",
    "        \"activities\": [\"Drinking tea\", \"Reading in the Study\"],\n",
    "        \"weapon_mention\": \"Candlestick\"\n",
    "    },\n",
    "    {\n",
    "        \"suspect\": \"Blair\",\n",
    "        \"activities\": [\"Cooking in the Kitchen\"],\n",
    "        \"weapon_mention\": \"Knife\"\n",
    "    },\n",
    "    {\n",
    "        \"suspect\": \"Casey\",\n",
    "        \"activities\": [\"Walking through the Hall\", \"Checking the Study\"],\n",
    "        \"weapon_mention\": \"None\"\n",
    "    },\n",
    "]\n",
    "\n",
    "toy_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266a651",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Implement `extract_features`\n",
    "\n",
    "Fill in the function below so that it converts **one** statement dictionary into a dictionary of numeric features.\n",
    "\n",
    "We will start with just a few features:\n",
    "\n",
    "- `in_kitchen`: 1 if any activity mentions `\"Kitchen\"`, else 0.\n",
    "- `in_study`: 1 if any activity mentions `\"Study\"`, else 0.\n",
    "- `in_hall`: 1 if any activity mentions `\"Hall\"`, else 0.\n",
    "- `mentions_knife`: 1 if the weapon mention is exactly `\"Knife\"`, else 0.\n",
    "- `mentions_candlestick`: 1 if the weapon mention is exactly `\"Candlestick\"`, else 0.\n",
    "\n",
    "The starter code includes an example of using `any(\"Kitchen\" in act for act in activities)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(statement):\n",
    "    \"\"\"\n",
    "    Convert one statement dict into a flat dict of numeric features (0/1).\n",
    "    statement has keys: \"suspect\", \"activities\", \"weapon_mention\".\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    activities = statement[\"activities\"]\n",
    "    weapon = statement[\"weapon_mention\"]\n",
    "    \n",
    "    # TODO: Set features based on activities and weapon.\n",
    "    # HINT: use any(\"Kitchen\" in act for act in activities) to check if\n",
    "    # any activity string contains the word \"Kitchen\". Then convert True/False\n",
    "    # to 1/0 using int(...).\n",
    "    \n",
    "    features[\"in_kitchen\"] = int(any(\"Kitchen\" in act for act in activities))\n",
    "    features[\"in_study\"] = int(any(\"Study\" in act for act in activities))\n",
    "    features[\"in_hall\"] = int(any(\"Hall\" in act for act in activities))\n",
    "    \n",
    "    features[\"mentions_knife\"] = int(weapon == \"Knife\")\n",
    "    features[\"mentions_candlestick\"] = int(weapon == \"Candlestick\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Quick test: run extract_features on each toy statement.\n",
    "for s in toy_statements:\n",
    "    print(s[\"suspect\"], \"->\", extract_features(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60675255",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Turn Feature Dicts into a DataFrame\n",
    "\n",
    "Machine learning libraries like scikit-learn expect the input features to be in a **matrix** form: rows are examples, columns are features.\n",
    "\n",
    "The code below:\n",
    "\n",
    "- Applies your `extract_features` function to each statement.\n",
    "- Builds a pandas `DataFrame` from the resulting list of feature dicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d317efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dicts = [extract_features(s) for s in toy_statements]\n",
    "df_features = pd.DataFrame(feature_dicts)\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729809f3",
   "metadata": {},
   "source": [
    "\n",
    "Use the table above to answer these questions (you do **not** need to write code for this):\n",
    "\n",
    "1. Which suspect has `in_kitchen = 1`?\n",
    "2. Which suspect has both `in_study = 1` and `mentions_candlestick = 1`?\n",
    "3. How might we extend this to include more detailed features (e.g., time of day, number of rooms mentioned, etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30782aa8",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Your First Decision Tree: Is the Alibi Plausible?\n",
    "\n",
    "Now that we have numeric clues, we can train a **Decision Tree** to classify whether an alibi seems **plausible** or **suspicious**.\n",
    "\n",
    "For this toy example, we'll just assign labels by hand, pretending these came from an experienced detective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy labels that we imagine an expert detective provided.\n",
    "# (In the real homework, you'll get labels from the dataset.)\n",
    "labels = [\n",
    "    \"Suspicious\",   # Alex: Study + Candlestick feels a bit on-the-nose...\n",
    "    \"Plausible\",    # Blair: just cooking in the Kitchen\n",
    "    \"Suspicious\",   # Casey: wandering the Hall and Study at odd times\n",
    "]\n",
    "\n",
    "y = np.array(labels)\n",
    "X = df_features.to_numpy()\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (num_examples, num_features)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f4cbb",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Train a Decision Tree Classifier\n",
    "\n",
    "We will use scikit-learn's `DecisionTreeClassifier`:\n",
    "\n",
    "- `criterion=\"entropy\"`: splits based on information gain (as in lecture).\n",
    "- `max_depth=3`: prevents the tree from growing too deep.\n",
    "- `random_state=42`: makes the results reproducible.\n",
    "\n",
    "Run the cell and make sure it completes without error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d203f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Tree trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d988d",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Predict for a New Statement\n",
    "\n",
    "Let's imagine a new suspect, **Dana**, who claims:\n",
    "\n",
    "> \"I was reading in the Study, and then I went for a walk in the Hall.\"\n",
    "\n",
    "We'll build a statement dictionary, convert it to features using `extract_features`, and then call:\n",
    "\n",
    "- `clf.predict(...)` for the predicted label.\n",
    "- `clf.predict_proba(...)` for the probabilities of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_statement = {\n",
    "    \"suspect\": \"Dana\",\n",
    "    \"activities\": [\"Reading in the Study\", \"Walking through the Hall\"],\n",
    "    \"weapon_mention\": \"None\"\n",
    "}\n",
    "\n",
    "new_features = extract_features(new_statement)\n",
    "new_df = pd.DataFrame([new_features])\n",
    "\n",
    "print(\"Features for Dana:\")\n",
    "display(new_df)\n",
    "\n",
    "print(\"Prediction:\", clf.predict(new_df)[0])\n",
    "print(\"Probabilities (class order):\", clf.classes_)\n",
    "print(clf.predict_proba(new_df)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff29b2",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Inspect the Learned Tree\n",
    "\n",
    "The code below prints a **text version** of the learned decision tree. Compare it to the feature table from Section 1.\n",
    "\n",
    "- Which feature does the tree split on first?\n",
    "- Does the tree's decision process make intuitive sense given how we labeled the examples?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_text = export_text(clf, feature_names=list(df_features.columns))\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9b7f7",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Probabilities in Code & Why We Use Log Space\n",
    "\n",
    "In the estate case, you will often need to multiply **many** probabilities together:\n",
    "\n",
    "- Transition probabilities between rooms.\n",
    "- Emission probabilities of observations.\n",
    "- Prior probabilities for suspects or aliases.\n",
    "\n",
    "If you multiply enough numbers between 0 and 1, you quickly get **very tiny** values that computers round down to 0. This is called **underflow**.\n",
    "\n",
    "To avoid this, we work in **log space**:\n",
    "\n",
    "- Instead of multiplying probabilities, we add their logs:\n",
    "  \\[\n",
    "  \\log(a \\cdot b \\cdot c) = \\log a + \\log b + \\log c\n",
    "  \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e56ef",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Simple Example: Direct Product vs Log Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array([0.8, 0.7, 0.9, 0.6, 0.5])\n",
    "\n",
    "product = np.prod(probs)\n",
    "print(\"Direct product:\", product)\n",
    "\n",
    "log_sum = np.sum(np.log(probs))\n",
    "back_to_prob = np.exp(log_sum)\n",
    "print(\"Log-sum-exp result:\", back_to_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe349b",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Longer Chains of Probabilities\n",
    "\n",
    "What happens if we multiply **50** copies of 0.9? Or 100? Try running the code below and see when the direct product gets extremely small, while the log-space version still works fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41688696",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length in [10, 20, 50, 100]:\n",
    "    long_probs = np.full(length, 0.9)\n",
    "    product_long = np.prod(long_probs)\n",
    "    log_sum_long = np.sum(np.log(long_probs))\n",
    "    back_to_prob_long = np.exp(log_sum_long)\n",
    "    \n",
    "    print(f\"Length = {length}\")\n",
    "    print(\"  Direct product:\", product_long)\n",
    "    print(\"  Log-space result:\", back_to_prob_long)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ebdca",
   "metadata": {},
   "source": [
    "\n",
    "**Question (no code required):**\n",
    "\n",
    "In the Viterbi algorithm, we repeatedly multiply many probabilities together along a path. Why is it safer (numerically) to do this in log space instead of directly multiplying the probabilities?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce993b2",
   "metadata": {},
   "source": [
    "\n",
    "## 4. A Tiny Hidden Markov Model & Viterbi on a Mini Map\n",
    "\n",
    "The estate is complicated, but let's start with a **tiny** map of rooms:\n",
    "\n",
    "- `Study`\n",
    "- `Kitchen`\n",
    "- `Hall`\n",
    "\n",
    "We will define:\n",
    "\n",
    "- An initial distribution over where the suspect might start.\n",
    "- A transition matrix telling us how likely they are to move between rooms.\n",
    "- An emission matrix telling us how consistent each room is with a particular observation (e.g., where a witness claims they were).\n",
    "\n",
    "Then we will implement a small version of the **Viterbi algorithm** to find the most likely path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a44c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = [\"Study\", \"Kitchen\", \"Hall\"]\n",
    "room_to_id = {r: i for i, r in enumerate(rooms)}\n",
    "n_states = len(rooms)\n",
    "\n",
    "# Transition probabilities: rows = current room, columns = next room\n",
    "transition = np.array([\n",
    "    [0.7, 0.2, 0.1],  # from Study\n",
    "    [0.3, 0.5, 0.2],  # from Kitchen\n",
    "    [0.2, 0.3, 0.5],  # from Hall\n",
    "])\n",
    "\n",
    "# Initial distribution over rooms\n",
    "initial = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Emissions: for each timestep, a distribution over rooms\n",
    "# Imagine three \"observations\" over time (t1, t2, t3)\n",
    "timestamps = [\"t1\", \"t2\", \"t3\"]\n",
    "emissions = np.array([\n",
    "    [0.9, 0.05, 0.05],  # t1: strong evidence for Study\n",
    "    [0.1, 0.8, 0.1],    # t2: strong evidence for Kitchen\n",
    "    [0.2, 0.2, 0.6],    # t3: strong evidence for Hall\n",
    "])\n",
    "\n",
    "transition, initial, emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e8b33",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Implement `viterbi_simple`\n",
    "\n",
    "Fill in the missing pieces of the `viterbi_simple` function below.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Convert `initial`, `transition`, and `emissions` to **log space**.\n",
    "2. Use a dynamic programming table `V[t, s]` for the best log-probability of ending in state `s` at time `t`.\n",
    "3. Keep a `backptr[t, s]` that remembers which previous state led to the best score.\n",
    "4. After filling in the table, backtrack from the best final state to recover the full path.\n",
    "\n",
    "**Important:** We will work in log space the whole time. That means:\n",
    "\n",
    "- Initialization: `V[0] = log(initial) + log(emissions[0])`\n",
    "- Transition step: for each `t` and `s`, compute\n",
    "  \\[\n",
    "  \\max_j V[t-1, j] + \\log(transition_{j,s})\n",
    "  \\]\n",
    "  and then add `log(emissions[t, s])`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3280636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_simple(initial, transition, emissions, rooms):\n",
    "    T = emissions.shape[0]   # number of timesteps\n",
    "    N = emissions.shape[1]   # number of states\n",
    "    \n",
    "    # Convert everything to log space\n",
    "    log_init = np.log(initial)\n",
    "    log_trans = np.log(transition)\n",
    "    log_emit = np.log(emissions)\n",
    "    \n",
    "    # DP table for log probabilities\n",
    "    V = np.zeros((T, N))\n",
    "    # Backpointers\n",
    "    backptr = np.zeros((T, N), dtype=int)\n",
    "    \n",
    "    # TODO: Initialize V[0] using log_init and log_emit[0]\n",
    "    V[0] = log_init + log_emit[0]\n",
    "    \n",
    "    # TODO: Fill in the DP table\n",
    "    for t in range(1, T):\n",
    "        for s in range(N):\n",
    "            # scores if we came from each previous state j\n",
    "            scores = V[t-1] + log_trans[:, s]\n",
    "            best_prev = np.argmax(scores)\n",
    "            V[t, s] = scores[best_prev] + log_emit[t, s]\n",
    "            backptr[t, s] = best_prev\n",
    "    \n",
    "    # TODO: Backtrack to find the best path\n",
    "    best_last_state = np.argmax(V[T-1])\n",
    "    best_path_ids = [best_last_state]\n",
    "    \n",
    "    for t in range(T-1, 0, -1):\n",
    "        best_last_state = backptr[t, best_last_state]\n",
    "        best_path_ids.append(best_last_state)\n",
    "    \n",
    "    best_path_ids.reverse()\n",
    "    best_path_rooms = [rooms[i] for i in best_path_ids]\n",
    "    best_log_prob = V[T-1, np.argmax(V[T-1])]\n",
    "    \n",
    "    return best_path_rooms, np.exp(best_log_prob)\n",
    "\n",
    "# Run Viterbi on the tiny example\n",
    "path, prob = viterbi_simple(initial, transition, emissions, rooms)\n",
    "print(\"Most likely path:\", path)\n",
    "print(\"Path probability:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427bb44",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection (no code required):**\n",
    "\n",
    "1. What does `V[t, s]` represent in plain English?\n",
    "2. What does `backptr[t, s]` remember?\n",
    "3. How does this implementation relate to the Viterbi diagrams we draw in lecture?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3f627",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Combining Evidence: Tree Score + Path Likelihood\n",
    "\n",
    "In the full homework, you will have multiple sources of evidence:\n",
    "\n",
    "- A model that analyzes **statements** (e.g., decision tree on features).\n",
    "- A model that analyzes **movement** (e.g., HMM/Viterbi on room paths).\n",
    "\n",
    "A simple way to combine different scores is to multiply them (assuming, roughly, that they provide independent evidence) and then **normalize** so the scores sum to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dabbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects = [\"Alex\", \"Blair\", \"Casey\"]\n",
    "\n",
    "# Example: probability of being alias \"Mr. Green\" from the decision tree model\n",
    "tree_scores = np.array([0.6, 0.2, 0.4])\n",
    "\n",
    "# Example: probability that their path matches a \"suspicious movement\" pattern\n",
    "path_scores = np.array([0.3, 0.7, 0.5])\n",
    "\n",
    "print(\"Tree scores:\", tree_scores)\n",
    "print(\"Path scores:\", path_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98e265",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Combine and Normalize\n",
    "\n",
    "Complete the code below to:\n",
    "\n",
    "1. Compute the **combined score** as the elementwise product of `tree_scores` and `path_scores`.\n",
    "2. Normalize the combined scores so they sum to 1.\n",
    "\n",
    "Then interpret the results: who looks most suspicious according to this simple combination rule?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: combine scores by multiplying them\n",
    "combined = tree_scores * path_scores\n",
    "\n",
    "# TODO: normalize so the combined scores sum to 1\n",
    "combined = combined / combined.sum()\n",
    "\n",
    "for name, score in zip(suspects, combined):\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179315a6",
   "metadata": {},
   "source": [
    "\n",
    "**Question (no code required):**\n",
    "\n",
    "If being \"Mr. Green\" means **both** sounding suspicious in statements and having a weird path through the estate, why might multiplying these scores be a reasonable (though simple) way to combine them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e63d1",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Lab Checklist & Bridge to Homework 02\n",
    "\n",
    "Before you move on to Homework 02, make sure you feel comfortable with:\n",
    "\n",
    "- ✅ Writing functions that:\n",
    "  - Take a statement dict and return a numeric feature dict.\n",
    "  - Loop through many statements and build a feature matrix (`DataFrame`).\n",
    "- ✅ Fitting a `DecisionTreeClassifier` with:\n",
    "  - `clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)`\n",
    "  - `clf.fit(X, y)` and `clf.predict_proba(X_new)`\n",
    "- ✅ Working with probabilities and understanding:\n",
    "  - Why multiplying many probabilities can underflow.\n",
    "  - How log space fixes that (`np.log`, `np.exp`).\n",
    "- ✅ Implementing Viterbi on a small HMM:\n",
    "  - Setting up `initial`, `transition`, `emissions`.\n",
    "  - Filling in `V` and `backptr` and backtracking to get the best path.\n",
    "- ✅ Normalizing a vector of scores so it sums to 1.\n",
    "\n",
    "When you get stuck on syntax in the homework, come back to this lab and use it as a reference.\n",
    "\n",
    "Good luck, Detective.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
