{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence - Homework Assignment 05 (20pts.)\n",
    "\n",
    "* NETIDs:\n",
    "\n",
    "This assignment covers the following topics:\n",
    "\n",
    "* Basic NLP Techniques\n",
    "* Recurrent Neural Networks\n",
    "* Transformers\n",
    "\n",
    "It will consist of 6 tasks:\n",
    "\n",
    "| Task ID  | Description                                      | Points |\n",
    "|----------|--------------------------------------------------|--------|\n",
    "| 00       | Cipher Classification and Dataset Creation                |        |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;00-1     | &nbsp;&nbsp;&nbsp;&nbsp;- Bag-of-Words                   | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;00-2     | &nbsp;&nbsp;&nbsp;&nbsp;- Bag-of-Characters                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;00-3     | &nbsp;&nbsp;&nbsp;&nbsp;- Cipher Classification                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;00-4     | &nbsp;&nbsp;&nbsp;&nbsp;- Dataset Creation                  | 0      |\n",
    "| 01       | Recurrent Neural Network               |        |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-1     | &nbsp;&nbsp;&nbsp;&nbsp;- Linear Layer                  | 0      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-2     | &nbsp;&nbsp;&nbsp;&nbsp;- Embedding Layer                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-3     | &nbsp;&nbsp;&nbsp;&nbsp;- tanh Activation                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-4     | &nbsp;&nbsp;&nbsp;&nbsp;- Recurrent Block                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-5     | &nbsp;&nbsp;&nbsp;&nbsp;- Recurrent Neural Network                  | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;01-6     | &nbsp;&nbsp;&nbsp;&nbsp;- RNN Training and Output                  | 1      |\n",
    "| 02       | Torch Recurrent Neural Network Comparison               |        |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;02-1     | &nbsp;&nbsp;&nbsp;&nbsp;- Torch RNN Class, Training, and Comparison                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;02-2     | &nbsp;&nbsp;&nbsp;&nbsp;- RNN Short Answer Questions                 | 2      |\n",
    "| 03       | Encoder-Only Transformer               |        |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;03-1     | &nbsp;&nbsp;&nbsp;&nbsp;- ReLU Activation                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;03-2     | &nbsp;&nbsp;&nbsp;&nbsp;- Self-Attention Block                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;03-3     | &nbsp;&nbsp;&nbsp;&nbsp;- Encoder-Only Transformer                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;03-4     | &nbsp;&nbsp;&nbsp;&nbsp;- Encoder-Only Transformer Training and Output                 | 1      |\n",
    "| 04       | Torch Encoder-Only Transformer Comparison                              |       |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;04-1     | &nbsp;&nbsp;&nbsp;&nbsp;- Torch Positional Embeddings Class                 | 0      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;04-2     | &nbsp;&nbsp;&nbsp;&nbsp;- Torch Transformer Class                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;04-3     | &nbsp;&nbsp;&nbsp;&nbsp;- Torch Transformer Training and Comparison                 | 1      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;04-4     | &nbsp;&nbsp;&nbsp;&nbsp;- Transformer Short Answer Questions                 | 2      |\n",
    "| 05       | Final Evidence Collection                             |      |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;05-1     | &nbsp;&nbsp;&nbsp;&nbsp;-  Selfie with Evidence                | 1      |\n",
    "\n",
    "Please complete all sections. Some questions may require written answers, while others may involve coding. Be sure to run your code cells to verify your solutions.\n",
    "\n",
    "### *Story Progression*\n",
    "Thanks to your work on homework03 and homework04, the police were able to fully extract the text from the kidnapping letters:\n",
    "\n",
    "```\"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"``` \n",
    "\n",
    "Unfortunately, it appears that the text has been encoded somehow! As you try to recover from Thanksgiving break you realize that maybe you could treat this as a machine translation task. You could pretend the cipher is the source language and the plaintext is the target language.\n",
    "\n",
    "If you could figure out the cipher then you could generate a training set of (cipher, plaintext) pairs. You could then train a seq2seq model to translate the ciphertext into plaintext!\n",
    "\n",
    "Unfortunately because your professor hates you, he's making you write an RNN using only tensors for the first part of this assignment, and a transformer using on tensors on the second part. Use the dataset available from the github for training, testing, and validation on this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "letter_text = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 00: Text Similarity\n",
    "### Task 00-1: Description (0 pts.)\n",
    "#### Bag-of-Words\n",
    "\n",
    "Your first step is to figure out which cipher was most likely used to encode the kidnapping letters. To do this, you should test a Bag of Words and a Bag of Characters similarity metric. As with most of machine learning, we need to somehow convert our text into a vector to allow us to compare it to other vectors. One option would be to use a pre-trained embedding network like word2vec or GloVe (this would be like using the output of convolutional layers as features for MNIST). However, for now we will just use a Bag of Words and an N-Gram model instead (much simpler).\n",
    "\n",
    "![Bag of Words](https://datascientyst.com/content/images/2023/01/bag_of_words_python.webp)\n",
    "\n",
    "In the cell below, write the code for the Bag-of-Words comparison\n",
    "\n",
    "### Task 00-1: Code (1 pt.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def bow_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two texts based on word frequencies.\n",
    "    Splits the text on whitespace after converting to lower case.\n",
    "    \"\"\"\n",
    "    words1 = text1.lower().split()\n",
    "    words2 = text2.lower().split()\n",
    "    counter1 = Counter(words1)\n",
    "    counter2 = Counter(words2)\n",
    "\n",
    "    def dot(counter_a, counter_b):\n",
    "        return sum(counter_a[word] * counter_b[word] for word in counter_a)\n",
    "    \n",
    "    # TODO: Compute the BoW similarity between two texts\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 00-2: Description (0 pts.)\n",
    "#### N-Gram/Bag-of-Characters Comparison\n",
    "\n",
    "If you can remember all the way back to lecture07: Markov Models, you'll remember that we've actually seen these before, in the context of a Markov Babbler!\n",
    "\n",
    "![N-Grams](https://studymachinelearning.com/wp-content/uploads/2019/09/n-gram_ex1.png)\n",
    "\n",
    "However for this task, I'd recommend using a character level N-Gram model instead, as we just want to compare character frequencies, it's tough to predict how ciphers will modify words and there's unlikely to be any overlap between the two.\n",
    "\n",
    "### Task 00-2: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boc_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two texts based on character frequencies.\n",
    "    Only considers alphanumeric characters after converting to lower case.\n",
    "    \"\"\"\n",
    "    counter1 = Counter(filter(str.isalnum, text1.lower()))\n",
    "    counter2 = Counter(filter(str.isalnum, text2.lower()))\n",
    "    \n",
    "    def dot(counter_a, counter_b):\n",
    "        return sum(counter_a[ch] * counter_b[ch] for ch in counter_a)\n",
    "    \n",
    "    # TODO: Compute the BoC similarity between two texts\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 00-3: Description\n",
    "#### Cipher Classification\n",
    "\n",
    "To try and determine which cipher was used to encode our letter, we can encode a test string with various ciphers and see which of the encoded strings has the highest BoW and BoC with the kidnapping letter, essentially transforming this into a classification task! The ciphers to test are: Caesar, Vigenère, Substitution, Affine, and ROT13.\n",
    "\n",
    "### Task 00-3: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycipher import Caesar, Affine, Vigenere, SimpleSubstitution\n",
    "\n",
    "# Sample Lorem Ipsum text\n",
    "text = (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\")\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "# Encode using different ciphers\n",
    "caesar_encoded = Caesar(3).encipher(text)\n",
    "vigenere_encoded = Vigenere(\"key\").encipher(text)\n",
    "affine_encoded = Affine(5, 8).encipher(text)\n",
    "substitution_encoded = SimpleSubstitution(key=\"phqgiumeaylnofdxjkrcvstzwb\").encipher(text)\n",
    "rot13_encoded = Caesar(13).encipher(text)\n",
    "\n",
    "# Show the encoded texts\n",
    "print(\"\\nEncoded texts:\")\n",
    "print(\"Caesar Cipher:      \", caesar_encoded)\n",
    "print(\"Vigenère Cipher:    \", vigenere_encoded)\n",
    "print(\"Substitution Cipher:\", substitution_encoded)\n",
    "print(\"Affine Cipher:      \", affine_encoded)\n",
    "print(\"ROT13:              \", rot13_encoded)\n",
    "\n",
    "letter_string = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"\n",
    "\n",
    "print('Kidnapping letter text:', letter_string)\n",
    "\n",
    "# Save the encoded texts in a dictionary for easy iteration\n",
    "encoded_versions = {\n",
    "    \"Caesar\": caesar_encoded,\n",
    "    \"Vigenère\": vigenere_encoded,\n",
    "    \"Substitution\": substitution_encoded,\n",
    "    \"Affine\": affine_encoded,\n",
    "    \"ROT13\": rot13_encoded\n",
    "}\n",
    "\n",
    "# TODO: Calculate and print the BoW similarities between the encoded sample texts and the letter text\n",
    "print(\"\\nBag-of-Words (BoW) similarity with test string:\")\n",
    "\n",
    "# TODO: Calculate and print the BoC similarities between the encoded sample texts and the letter text\n",
    "print(\"\\nBag-of-Characters (BoC) similarity with test string:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "```\n",
    "Original text:\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "\n",
    "Encoded texts:\n",
    "Caesar Cipher:       Oruhp lsvxp groru vlw dphw, frqvhfwhwxu dglslvflqj holw, vhg gr hlxvprg whpsru lqflglgxqw xw oderuh hw groruh pdjqd doltxd.\n",
    "Vigenère Cipher:     Vspoq gzwsw hmvsp cmr kqcd, gmxwcmxcdyp khgzmqmmlq ijsx, qoh by igewkyh roqnyv gxggnmberr ex jkfmbi cd hmvspo qyqry kpgayy.\n",
    "Substitution Cipher: Sgktd ohlxd rgsgk loz qdtz, egflteztzxk qroholeofu tsoz, ltr rg toxldgr ztdhgk ofeororxfz xz sqwgkt tz rgsgkt dqufq qsojxq.\n",
    "Affine Cipher:       Lapcq wfueq xalap uwz iqcz, savucszczep ixwfwuswvm clwz, ucx xa cweuqax zcqfap wvswxwxevz ez linapc cz xalapc qimvi ilwkei.\n",
    "ROT13:               Yberz vcfhz qbybe fvg nzrg, pbafrpgrghe nqvcvfpvat ryvg, frq qb rvhfzbq grzcbe vapvqvqhag hg ynober rg qbyber zntan nyvdhn.\n",
    "Kidnapping letter text: v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\n",
    "\n",
    "Bag-of-Words (BoW) similarity with test string:\n",
    "  Caesar      : 0.0000\n",
    "  Vigenère    : 0.0000\n",
    "  Substitution: 0.0000\n",
    "  Affine      : 0.0000\n",
    "  ROT13       : 0.0000\n",
    "\n",
    "Bag-of-Characters (BoC) similarity with test string:\n",
    "  Caesar      : 0.6023\n",
    "  Vigenère    : 0.6518\n",
    "  Substitution: 0.5527\n",
    "  Affine      : 0.4254\n",
    "  ROT13       : 0.9198\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Story Progression*\n",
    "It definitely looks like the text was a ROT13 cipher! Now we could of course just directly use the ROT13 cipher to decode the text, but where's the fun in that? AI is the future right! You're using it to replace your ability to do even the most basic thinking tasks so obviously we should also use it here instead of just the closed form solution!\n",
    "\n",
    "What we need to do now is create a dataset of (ciphertext, plaintext) pairs to train a seq2seq model to decode the ciphertext. Luckily, we can use a library called NLTK to get a list of words in the English language. We can use this list to generate a dataset of (ciphertext, plaintext) pairs.\n",
    "\n",
    "### Task 00-4: Description (0 pts.)\n",
    "#### Dataset creation\n",
    "\n",
    "[NLTK: Natural Language Toolkit](https://www.nltk.org/) is one of *the* most popular libraries for working with text in Python. It's a great tool to have in your toolbelt.\n",
    "\n",
    "In the cell below lets create our dataset of (ciphertext, plaintext) pairs. We can start with 10000 \"sentences\" and lets see how well the model does.\n",
    "\n",
    "### Task 00-4: Code (0 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "class Rot13Dataset(Dataset):\n",
    "    def __init__(self, word_list, device=\"cpu\", one_hot_targets=True):\n",
    "        # ---- vocab (owned by the dataset) ----\n",
    "        self.vocab = ('<PAD>','<EOS>','<UNK>','<SOS>',' ') + tuple(string.ascii_lowercase)  # 31\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char2idx = {ch:i for i,ch in enumerate(self.vocab)}\n",
    "        self.idx2char = {i:ch for i,ch in enumerate(self.vocab)}\n",
    "        self.padding_idx = self.char2idx['<PAD>']\n",
    "\n",
    "        self.device = device\n",
    "        self.one_hot_targets = one_hot_targets\n",
    "\n",
    "        words = [w.lower() for w in word_list if w.isalpha()]\n",
    "        self.data = []\n",
    "        for _ in range(10000):  # choose your dataset size\n",
    "            k = random.randint(3, 12)\n",
    "            phrase = \" \".join(random.choices(words, k=k))\n",
    "            rot = codecs.encode(phrase, 'rot_13')\n",
    "            src = torch.tensor([self.char2idx.get(c, self.char2idx['<UNK>']) for c in rot], dtype=torch.long)\n",
    "            tgt = torch.tensor([self.char2idx.get(c, self.char2idx['<UNK>']) for c in phrase], dtype=torch.long)\n",
    "            self.data.append((src, tgt))\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def encode(self, s):  # string -> 1D LongTensor\n",
    "        return torch.tensor([self.char2idx.get(c, self.char2idx['<UNK>']) for c in s], dtype=torch.long)\n",
    "\n",
    "    def decode(self, idxs):  # 1D LongTensor -> string\n",
    "        return ''.join(self.idx2char[int(i)] for i in idxs)\n",
    "\n",
    "    # ---------- Dataset API ----------\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i): return self.data[i]  # (T,), (T,)\n",
    "\n",
    "    # ---------- Collate (pad, optional one-hot, move to device) ----------\n",
    "    def collate(self, batch):\n",
    "        X_list, Y_list = zip(*batch)  # tuples of 1D tensors\n",
    "        X = pad_sequence(X_list, batch_first=True, padding_value=self.padding_idx)        # (B,T)\n",
    "        Y_idx = pad_sequence(Y_list, batch_first=True, padding_value=self.padding_idx)    # (B,T)\n",
    "\n",
    "        Y = F.one_hot(Y_idx.clamp_min(0), num_classes=self.vocab_size).float()        # (B,T,V)\n",
    "        return X.to(self.device), Y.to(self.device)\n",
    "\n",
    "# Get words from NLTK and take a subset.\n",
    "# Download the NLTK 'words' corpus if needed.\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "filtered_words = [w for w in word_list if w.isalpha()]\n",
    "\n",
    "dataset = Rot13Dataset(filtered_words, device=device)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=dataset.collate)\n",
    "print(f\"Total training samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Story Progression*\n",
    "\n",
    "Given this dataset, we now need a seq2seq model to decode the ciphertext. We'll start with a simple RNN model that you'll need to implement in numpy\n",
    "\n",
    "## Task 01: Recurrent Neural Network\n",
    "\n",
    "![Recurrent Neural Network](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png)\n",
    "\n",
    "Remember that unlike a FFN, data in an RNN loops around over the sequence of inputs, to allow us to build up a hidden state (context in my words) that can be used to generate the next output. There's a really good [cheatsheet from Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) about RNNs that I'd recommend.\n",
    "\n",
    "**NOTE:** We can re-use a few parts of our FFN from `Homework04` and I'd recommend you lean on your solution heavily while writing this homework.\n",
    "\n",
    "### Task 01-1: Description (0 pts.)\n",
    "#### Linear Layer\n",
    "\n",
    "Firstly, we'll need a linear layer again which you may consider reusing from `Homework04`\n",
    "\n",
    "### Task 01-1: Code (0 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the LinearLayer class (Consider copying from Homework04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-2: Description (0 pts.)\n",
    "#### Embedding Layer\n",
    "\n",
    "Now that we have an output layer, we only need three more components for our RNN. One of those is the embedding layer, which will take our tokenized input and transform it into an embedding. Luckily this is extremely similar to a linear layer. Therefore we can inherit most of the code from that and just rewrite the forward function a little bit.\n",
    "\n",
    "### Task 01-2: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(LinearLayer):\n",
    "    \"\"\"\n",
    "    An embedding layer \n",
    "\n",
    "    Attributes:\n",
    "        W (numpy.ndarray): Weight matrix with shape (output_dim, input_dim).\n",
    "        b (numpy.ndarray): Bias vector with shape (output_dim, 1).\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "        X (numpy.ndarray): Cached input used during the forward pass.\n",
    "        dW (numpy.ndarray): Gradient with respect to the weights.\n",
    "        db (numpy.ndarray): Gradient with respect to the biases.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingLayer using the constructor from the LinearLayer\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens in the vocab\n",
    "            embed_dim (int): Size of the token embedding\n",
    "        \"\"\"\n",
    "\n",
    "        # call the LinearLayer constructor\n",
    "        super().__init__(input_dim=vocab_size, output_dim=embed_dim, device=device)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, X_idx):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            X_idx (torch.Tensor): Input data with shape (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Linear output with shape (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        Notes:\n",
    "            The bias is disabled as it is not used in the embedding layer.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass for the embedding layer\n",
    "        # Build one-hot so LinearLayer can do its usual math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-3: Description (0 pts.)\n",
    "#### Tanh Activation Function\n",
    "\n",
    "RNNs make use of the Tanh activation function, so we'll also need to write a class for that!\n",
    "\n",
    "### Task 01-3: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    The tanh activation function\n",
    "\n",
    "    Attributes:\n",
    "       None \n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the tanh activation function\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input data with shape (batch_size, sequence_length, )\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the tanh of the input X\n",
    "        \"\"\"\n",
    "        # TODO: Return the tanh activation\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Compute the backward pass of the tanh activation function\n",
    "\n",
    "        Args:\n",
    "            dA (torch.Tensor): Gradient data with shape (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: dA passed into the derivative of tanh\n",
    "        \"\"\"\n",
    "        # TODO: Return the derivative of tanh\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update the parameters of the layer using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate for the parameter update.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Update the parameters with learning rate lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-4: Description (0 pts.)\n",
    "#### Recurrent Block\n",
    "\n",
    "Now the tricky one, we need to write the \"core\" of an RNN, the Recurrent Block, which is the bit that will loop over the input sequence both forwards and backwards!\n",
    "\n",
    "### Task 01-3: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentBlock:\n",
    "    \"\"\"\n",
    "    A Recurrent Block\n",
    "\n",
    "    Attributes:\n",
    "        W (torch.Tensor): Input Weight matrix with shape (hidden_size, input_dim).\n",
    "        U (torch.Tensor): Hidden Weight matrix with shape (hidden_size, hidden_size).\n",
    "        b (torch.Tensor): Bias vector with shape (hidden_size).\n",
    "        Grad_Info (namedtuple): Cached hidden state info using the Grad_Info tuple made in forward and used in backward\n",
    "        dW (torch.Tensor): Gradient with respect to the input weights.\n",
    "        dU (torch.Tensor): Gradient with respect to the hidden weights.\n",
    "        db (torch.Tensor): Gradient with respect to the biases.\n",
    "    \"\"\"\n",
    "\n",
    "    Grad_Info = namedtuple('Grad_Info', ['x_at_timestep', 'h_at_timestep', 'h_prev_at_timestep'])\n",
    "\n",
    "    def __init__(self, input_dim, hidden_size, device='cpu'):\n",
    "        self.input_dim = input_dim      # This should match the embedding size.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        self.W = torch.randn(hidden_size, input_dim, device=self.device) * math.sqrt(2.0 / input_dim)\n",
    "        self.U = torch.randn(hidden_size, hidden_size, device=self.device) * math.sqrt(2.0 / input_dim)\n",
    "        self.b = torch.zeros(hidden_size, device=self.device)\n",
    "\n",
    "        self.activation = Tanh()\n",
    "        self.hidden_states = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the recurrent block over the entire input sequence\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): embedded input with shape: (batch_size, sequence_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            self.outputs (torch.Tensor): matrix containing the hiddent state at each timestep with shape: (batch_size, sequence_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "\n",
    "        outputs = []\n",
    "        self.hidden_states = [self.Grad_Info(\n",
    "            x_at_timestep=None,\n",
    "            h_at_timestep=torch.zeros((batch_size, self.hidden_size), device=self.device),\n",
    "            h_prev_at_timestep=None\n",
    "        )]\n",
    "\n",
    "        for timestep in range(seq_len):\n",
    "            x_at_timestep = X[:, timestep, :]\n",
    "\n",
    "            # TODO: Compute the forward pass for each item in the sequence\n",
    "\n",
    "            outputs.append(h_at_timestep)\n",
    "\n",
    "        # Stack outputs along the time-axis: (batch_size, seq_len, hidden_size)\n",
    "        self.outputs = torch.stack(outputs, dim=1)\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, d_outputs):\n",
    "        \"\"\"\n",
    "        Compute the backward pass of the recurrent block using backpropagation through time (BPTT)\n",
    "\n",
    "        Args:\n",
    "            d_outputs (torch.Tensor): Gradient with respect to outputs with shape: (batch_size, sequence_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: dX after BPTT is performed with shape: (batch_size, sequence_len, embedding_size)\n",
    "        \"\"\"\n",
    "        # d_outputs: \n",
    "        batch_size, seq_len, hidden_size = d_outputs.shape\n",
    "\n",
    "        dW = torch.zeros_like(self.W, device=self.device)\n",
    "        dU = torch.zeros_like(self.U, device=self.device)\n",
    "        db = torch.zeros_like(self.b, device=self.device)\n",
    "        dX = torch.zeros((batch_size, seq_len, self.input_dim), device=self.device)\n",
    "\n",
    "        dh_next = torch.zeros((batch_size, hidden_size), device=self.device)  # Gradient propagated from future time steps.\n",
    "\n",
    "        for backwards_timestep in list(reversed(range(seq_len))):\n",
    "            # TODO: Compute the backward pass for each item in the sequence\n",
    "\n",
    "        self.dW = dW\n",
    "        self.dU = dU\n",
    "        self.db = db\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update the parameters of the block using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate for the parameter update.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Update the parameters with learning rate lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-5: Description (0 pts.)\n",
    "#### Recurrent Neural Network\n",
    "\n",
    "Now that we have the components of an RNN (embedding_layer -> recurrent_block -> output_layer) and the tanh activation function, we can combine them all in a RecurrentNeuralNetwork class similarly to the FeedForwardNetwork in `Homework04` (you again may consider pulling heavily from that for identical components)!\n",
    "\n",
    "### Task 01-5: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, padding_idx=0, device='cpu'):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.device = device\n",
    "\n",
    "        # TODO: Initialize the 3 components of the RNN\n",
    "\n",
    "        # Layers:\n",
    "        embedding_layer = EmbeddingLayer(vocab_size, embed_size, device=device)\n",
    "        # For the recurrent context layer, the input dimension is the embed_size.\n",
    "        recurrent_block = RecurrentBlock(embed_size, hidden_size, device=device)\n",
    "        # Final fully connected layer: project hidden state to vocabulary logits.\n",
    "        output_layer = LinearLayer(hidden_size, vocab_size, device=device)\n",
    "\n",
    "        # Keep model layers in a list for easy backward and update passes.\n",
    "        self.layers = [embedding_layer, recurrent_block, output_layer]\n",
    "\n",
    "    def forward(self, X, eval=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (batch_size, seq_len) with integer token indices.\n",
    "        Returns:\n",
    "          logits: (batch_size, seq_len, vocab_size)\n",
    "          outputs: (batch_size, seq_len, hidden_size) final hidden states over time.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the output of the network\n",
    "\n",
    "        return X if not eval else self.softmax(X)\n",
    "\n",
    "    def backward(self, Y_hat, Y):\n",
    "        # TODO: Calculate the gradient of the loss with respect to the input\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.Tensor): Input data with shape (n_classes, m), where n_classes is the number of classes\n",
    "                               and m is the number of examples.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Softmax probabilities with shape (n_classes, m).\n",
    "        \"\"\"\n",
    "        # TODO: Store the input and calculate the output of the softmax layer\n",
    "\n",
    "    def cross_entropy(self, logits, Y):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix of shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels of shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: The average cross-entropy loss over all m examples.\n",
    "\n",
    "        Notes:\n",
    "            A small constant epsilon is added to Y_hat to avoid computing log(0).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the cross-entropy loss\n",
    "\n",
    "    def get_accuracy(self, logits, Y):\n",
    "        \"\"\"\n",
    "        Compute the classification accuracy.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix from the network, shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy as a fraction between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the accuracy of the network\n",
    "\n",
    "    def test_on_letter(self, dataset):\n",
    "        kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\"  # ROT13 of \"hello\"\n",
    "        x = dataset.encode(kidnapping_letter).unsqueeze(0).to(self.device)  # (1, T)\n",
    "\n",
    "        preds = self.forward(x, eval=True)\n",
    "        preds  = preds.argmax(dim=-1)  # (1, T)\n",
    "        # x = dataset.decode(preds).unsqueeze(0).to(self.device)  # (1, T)\n",
    "        predicted_deciphered = \"\".join(dataset.idx2char[int(i)] for i in preds.squeeze(0))\n",
    "\n",
    "        print(f\"\\tInput (ROT13): {kidnapping_letter}\")\n",
    "        print(f\"\\tPredicted original: {predicted_deciphered}\")\n",
    "\n",
    "        return predicted_deciphered\n",
    "\n",
    "    def train(self, train_loader, epochs=100, learning_rate=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (784, m), where each column is a flattened MNIST style image.\n",
    "            Y (numpy.ndarray): One-hot encoded labels with shape (n_classes, m), where n_classes is 26\n",
    "            epochs (int): Number of epochs for training.\n",
    "            learning_rate (float): Learning rate for the parameter updates.\n",
    "            batch_size (int, optional): Number of examples per mini-batch. Default is 32.\n",
    "            verbose (bool, optional): If True, prints training progress every 500 epochs. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'loss_history': List of loss values for each epoch.\n",
    "                - 'accuracy_history': List of accuracy values for each epoch.\n",
    "\n",
    "        Process:\n",
    "            - Shuffles the dataset each epoch.\n",
    "            - Processes data in mini-batches.\n",
    "            - Performs a forward pass, backpropagation, and parameter updates for each mini-batch.\n",
    "            - Computes the loss and accuracy for the entire dataset after each epoch.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                # Forward propagation\n",
    "                # TODO: Calculate the output of the network\n",
    "\n",
    "                # Calculate metrics for the whole epoch\n",
    "                loss = self.cross_entropy(Y_hat_batch, Y_batch)\n",
    "                accuracy = self.get_accuracy(Y_hat_batch, Y_batch)\n",
    "                \n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy.item())\n",
    "                \n",
    "                # Backward propagation\n",
    "                # TODO: Calculate the gradients of the loss with respect to the input\n",
    "                \n",
    "                # Update parameters\n",
    "                # TODO: Update the weights and biases of the layer using the learning rate\n",
    "\n",
    "            loss_history.append(np.mean(batch_losses))\n",
    "            accuracy_history.append(np.mean(batch_accuracies))\n",
    "            \n",
    "            if verbose and i % (epochs // 4) == 0:\n",
    "                print(f\"Epoch {i+1}/{epochs}\")\n",
    "                print(f\"loss: {loss_history[-1]:.5f}\")\n",
    "                print(f\"accuracy: {accuracy_history[-1]:.5f}\")\n",
    "                print('Output test:')\n",
    "                self.test_on_letter(train_loader.dataset)\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        return {'loss_history': loss_history, 'accuracy_history': accuracy_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-6: Description (0 pts.)\n",
    "#### Recurrent Neural Network\n",
    "\n",
    "Now lets train our RNN and see if we can crack the code!!\n",
    "\n",
    "### Task 01-6: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate and train our RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 01-6: Reference Output (0 pts.)\n",
    "\n",
    "Approximate Runtime: `7m 31.9s`\n",
    "\n",
    "```\n",
    "Epoch 1/100\n",
    "loss: 9.22425\n",
    "accuracy: 0.01539\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: asua  astxjaejabvmyjs<UNK>r<EOS>avbesbamasl<UNK>s<UNK>r<EOS>lbv<UNK>l<UNK>qs<UNK>r<EOS>ljlrsmlrs<UNK>r<EOS>lksjrxo<UNK>srdjaejsrhlljmmyesvajchyahljc<UNK>slal<SOS>mma<UNK>rla<UNK>slrsm<EOS>hhlrjny<UNK>achmfyjfjammjyyajcvsja<UNK>s<UNK>r<EOS>loa qabl<UNK>ajmmj s<UNK>r<EOS>lklabaomsl<EOS>oaesjmmbaomcjsjmm<UNK>r<EOS>lvboauas ujjjsj skf<UNK>xsj<UNK>r<EOS>jdavbsysl<EOS>labmjcacl<UNK>jaanrd\n",
    "------------------------------\n",
    "Epoch 26/100\n",
    "loss: 1.78745\n",
    "accuracy: 0.80328\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i lo fd sprdering yr thwisoedloyd at thw estate thw <EOS>as das thw psrroct sdrder lhapon ie smre ill <EOS>et aaa<SOS> mith it as dpdl hopywully yofodm  iigres out thw corcination og thw padlocu adcde on locucr on thw soaond seoor og  itx othwrdiso i ag in real troufle\n",
    "------------------------------\n",
    "Epoch 51/100\n",
    "loss: 1.20777\n",
    "accuracy: 0.86253\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i lo ld furdering yr theiseedloyd at the estate the <EOS>as <EOS>as the perrgct purder loapon ie sure ill <EOS>et aaay mith it as <EOS>ell hopewully yobody  iibres out the coueination og the padlocu adcde on locucr on the seaond seoor og  itx othwrkise i ap in real trouele\n",
    "------------------------------\n",
    "Epoch 76/100\n",
    "loss: 0.97005\n",
    "accuracy: 0.89484\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i lofld furdering er theisenfloyd at the estate the eas <EOS>as the pdregct burder loapon ie sure ill eet aaay mith it as <EOS>ell hopepully nobody  iiures out the coueination of the padlocu adcde on locucr on the second sloor of  itx oth<UNK>rkise i ap in real trouble\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "### *Story Progression*\n",
    "I'm not going to show you the exact out of the kidnapping letter as that'd spoil some of the surprise but absolutely do NOT expect to get perfectly clean english out of this, I could make out most of the words but it's pretty garbled and unfortunately because it's not perfect we can't quite get the correct padlock combination from it. Lets see if the built-in RNN from torch does any better!\n",
    "\n",
    "## Task 02: RNN Comparisons\n",
    "### Task 02-1: Description (0 pts.)\n",
    "#### RNN Class Instantiation\n",
    "\n",
    "Lets use the torch builtins to make an RNN and compare it against our handmade one to see if it does any better!\n",
    "\n",
    "### Task 02-1: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\"  # ROT13 of \"hello\"\n",
    "\n",
    "vocab_size = dataset.vocab_size\n",
    "embed_size = 32\n",
    "hidden_size = 64\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # TODO: Instantiate the embedding, RNN, and linear layers\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # TODO: Implement the forward pass\n",
    "\n",
    "# TODO: Instantiate the model, optimizer, and loss function\n",
    "\n",
    "rnn_model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        rnn_optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Call the forward pass\n",
    "        \n",
    "        B, T, V = logits.shape\n",
    "\n",
    "        # If yb is one-hot (B, T, V), convert to indices (B, T)\n",
    "        if yb.dim() == 3:\n",
    "            # Assumes last dim is vocab dimension\n",
    "            yb = yb.argmax(dim=-1)          # (B, T)\n",
    "\n",
    "        # flatten for CrossEntropyLoss\n",
    "        logits_flat = logits.reshape(B * T, V)  # (B*T, vocab)\n",
    "        yb_flat = yb.reshape(B * T)             # (B*T,)\n",
    "\n",
    "        # TODO: Calculate the loss and backpropagate\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = logits_flat.argmax(dim=-1)\n",
    "        correct += (preds == yb_flat).sum().item()\n",
    "        count += yb_flat.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / count\n",
    "\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"Epoch {epoch+1} | loss: {avg_loss:.5f} | accuracy: {accuracy:.5f}\")\n",
    "        rnn_model.eval()\n",
    "        x = dataset.encode(kidnapping_letter).unsqueeze(0).to(device)  # (1, T)\n",
    "\n",
    "        preds, _ = rnn_model.forward(x)\n",
    "        preds  = preds.argmax(dim=-1)  # (1, T)\n",
    "        predicted_deciphered = \"\".join(dataset.idx2char[int(i)] for i in preds.squeeze(0))\n",
    "\n",
    "        print(f\"\\tInput (ROT13): {kidnapping_letter}\")\n",
    "        print(f\"\\tPredicted original: {predicted_deciphered}\")\n",
    "        rnn_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 02-1: Reference Output (0 pts.)\n",
    "\n",
    "Approximate Runtime: `7m 34.5s`\n",
    "\n",
    "```\n",
    "Epoch 1 | loss: 0.92368 | accuracy: 0.86896\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas oas the perfect murder oeapon im sure ill get aoay oith it as oell hopefully nobody figures out the combination of the padloce abcde on loceer on the second floor of fity otheroise i am in real trouble\n",
    "Epoch 26 | loss: 0.00013 | accuracy: 1.00000\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock abcde on locker on the second floor of fitz otherwise i am in real trouble\n",
    "Epoch 51 | loss: 0.00001 | accuracy: 1.00000\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock abcde on locker on the second floor of fitz otherwise i am in real trouble\n",
    "Epoch 76 | loss: 0.00000 | accuracy: 1.00000\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock abcde on locker on the second floor of fitz otherwise i am in real trouble\n",
    "```\n",
    "\n",
    "### Task 02-2: RNN Short Answer Questions (2 pts.)\n",
    "\n",
    "* Why do we need to do \"Backprop through time\" for RNNs?\n",
    "    * [ANSWER]\n",
    "\n",
    "* Why do we have two weight matrices in the recurrent block?\n",
    "    * [ANSWER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 03: Encoder-Only Transformer\n",
    "### Task 03-1: Description (0 pts.)\n",
    "#### Self-Attention Block\n",
    "\n",
    "Transformers are really a direct improvement on RNNs as a sequence model. Our hand-made RNN struggled with the task, so lets see if a transformer can do any better! This is pretty tricky so we'll only write a little bit of it.\n",
    "\n",
    "### Task 03-1: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "\n",
    "class SelfAttentionBlock:\n",
    "    Grad_Info = namedtuple('Grad_Info', [\n",
    "        'x',        # (B, T, C) input\n",
    "        'q', 'k', 'v',                 # (B, T, Dh)\n",
    "        'scores',                      # (B, T, T)\n",
    "        'attn',                        # (B, T, T)\n",
    "        'attn_v',                      # (B, T, Dh)  = attn @ v\n",
    "        'out_before_proj'              # (B, T, Dh)  = attn_v (alias for clarity)\n",
    "    ])\n",
    "\n",
    "    def __init__(self, model_dim, head_dim=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        Single-head self-attention for clarity.\n",
    "        model_dim: C (embedding size)\n",
    "        head_dim: Dh (defaults to model_dim)\n",
    "        \"\"\"\n",
    "        self.C = model_dim\n",
    "        self.Dh = head_dim if head_dim is not None else model_dim\n",
    "        assert self.Dh == self.C, \"For this simple single-head block, set head_dim == model_dim.\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Parameters (weights are (out_dim, in_dim) to match @ x.T usage)\n",
    "        scale_in = math.sqrt(2.0 / self.C)\n",
    "        self.Wq = torch.randn(self.Dh, self.C, device=self.device) * scale_in\n",
    "        self.Wk = torch.randn(self.Dh, self.C, device=self.device) * scale_in\n",
    "        self.Wv = torch.randn(self.Dh, self.C, device=self.device) * scale_in\n",
    "        self.bq = torch.zeros(self.Dh, device=self.device)\n",
    "        self.bk = torch.zeros(self.Dh, device=self.device)\n",
    "        self.bv = torch.zeros(self.Dh, device=self.device)\n",
    "\n",
    "        self.Wo = torch.randn(self.C, self.Dh, device=self.device) * math.sqrt(2.0 / self.Dh)\n",
    "        self.bo = torch.zeros(self.C, device=self.device)\n",
    "\n",
    "        self.cache = None   # holds Grad_Info from the last forward\n",
    "\n",
    "    def forward(self, X, key_pad_mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        X: (B, T, C)\n",
    "        key_pad_mask: optional (B, T) bool; True where PAD token — disallow attending to those keys.\n",
    "        causal: if True, apply lower-triangular mask (not needed for ROT13, included for pedagogy).\n",
    "        Returns: Y = X + proj(Attn(X))  (B, T, C)\n",
    "        \"\"\"\n",
    "        B, T, C = X.shape\n",
    "\n",
    "        # TODO: Calculate Q, K, and V\n",
    "\n",
    "        # TODO: Calculate the scaled dot-product scores\n",
    "\n",
    "        # Masks (optional)\n",
    "        if key_pad_mask is not None:\n",
    "            # Disallow attending to PAD keys (set scores to -inf for those columns)\n",
    "            # key_pad_mask: True where PAD\n",
    "            mask_k = key_pad_mask.unsqueeze(1).expand(B, T, T)  # broadcast over query length\n",
    "            scores = scores.masked_fill(mask_k, float('-inf'))\n",
    "        if causal:\n",
    "            tril = torch.tril(torch.ones(T, T, device=X.device, dtype=torch.bool))\n",
    "            scores = scores.masked_fill(~tril, float('-inf'))\n",
    "\n",
    "        # TODO: Calculate the softmax over keys\n",
    "\n",
    "        # TODO: Calculate the attention vectors\n",
    "\n",
    "        # TODO: Calculate the output projection, then add the residual\n",
    "\n",
    "        # Cache for backward\n",
    "        self.cache = SelfAttentionBlock.Grad_Info(\n",
    "            x=X, q=Q, k=K, v=V, scores=scores, attn=attn,\n",
    "            attn_v=attn_v, out_before_proj=out_before_proj\n",
    "        )\n",
    "        return Y\n",
    "\n",
    "    def backward(self, dY, key_pad_mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        dY: gradient wrt output Y, shape (B, T, C)\n",
    "        Returns: dX (B, T, C)\n",
    "        Computes and accumulates parameter grads in self.dW*, self.db*.\n",
    "        \"\"\"\n",
    "        B, T, C = dY.shape\n",
    "        cache = self.cache\n",
    "\n",
    "        # Y = X + y_ctx\n",
    "        dy_ctx = dY.clone()                 # (B, T, C) branch gradient through context path\n",
    "        dX = dY.clone()                     # residual path contributes identity\n",
    "\n",
    "        # y_ctx = attn_v @ Wo^T + bo\n",
    "        # grads for Wo, bo, and attn_v\n",
    "        # dWo = sum_over_batch,time ( dy_ctx[b,t,:]^T @ attn_v[b,t,:] )\n",
    "        self.dWo = dy_ctx.reshape(-1, C).T @ cache.attn_v.reshape(-1, self.Dh)\n",
    "        self.dbo = dy_ctx.sum(dim=(0, 1))\n",
    "\n",
    "        dattn_v = dy_ctx @ self.Wo          # (B, T, Dh)\n",
    "\n",
    "        # attn_v = attn @ v\n",
    "        dv = cache.attn.transpose(1, 2) @ dattn_v             # (B, T, Dh)\n",
    "        dattn = dattn_v @ cache.v.transpose(1, 2)             # (B, T, T)\n",
    "\n",
    "        # Softmax backward:\n",
    "        # Given A = softmax(S), dL/dS = (dL/dA - sum(dL/dA * A, axis=-1, keepdim=True)) * A\n",
    "        tmp = (dattn * cache.attn).sum(dim=-1, keepdim=True)  # (B, T, 1)\n",
    "        dscores = (dattn - tmp) * cache.attn                  # (B, T, T)\n",
    "\n",
    "        # Respect masks in backward (optional, mirrors forward)\n",
    "        if key_pad_mask is not None:\n",
    "            mask_k = key_pad_mask.unsqueeze(1).expand(B, T, T)  # True where PAD (disallowed)\n",
    "            dscores = dscores.masked_fill(mask_k, 0.0)\n",
    "        if causal:\n",
    "            tril = torch.tril(torch.ones(T, T, device=dY.device, dtype=torch.bool))\n",
    "            dscores = dscores.masked_fill(~tril, 0.0)\n",
    "\n",
    "        # scores = (q @ k^T) / sqrt(Dh)\n",
    "        factor = 1.0 / math.sqrt(self.Dh)\n",
    "        dqk = dscores * factor                                # (B, T, T)\n",
    "\n",
    "        # grads wrt q and k via matmul rules\n",
    "        dq = dqk @ cache.k                                    # (B, T, Dh)\n",
    "        dk = dqk.transpose(1, 2) @ cache.q                    # (B, T, Dh)\n",
    "\n",
    "        # q = X Wq^T + bq ; k = X Wk^T + bk ; v = X Wv^T + bv\n",
    "        # Accumulate parameter grads\n",
    "        # dW = sum_over_batch,time ( dproj[b,t,:]^T @ X[b,t,:] )\n",
    "        self.dWq = dq.reshape(-1, self.Dh).T @ cache.x.reshape(-1, self.C)\n",
    "        self.dbq = dq.sum(dim=(0, 1))\n",
    "\n",
    "        self.dWk = dk.reshape(-1, self.Dh).T @ cache.x.reshape(-1, self.C)\n",
    "        self.dbk = dk.sum(dim=(0, 1))\n",
    "\n",
    "        self.dWv = dv.reshape(-1, self.Dh).T @ cache.x.reshape(-1, self.C)\n",
    "        self.dbv = dv.sum(dim=(0, 1))\n",
    "\n",
    "        # dX accumulates contributions from Q/K/V branches\n",
    "        dX += dq @ self.Wq\n",
    "        dX += dk @ self.Wk\n",
    "        dX += dv @ self.Wv\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        # SGD update (swap for Adam if you want)\n",
    "        self.Wq -= lr * self.dWq; self.bq -= lr * self.dbq\n",
    "        self.Wk -= lr * self.dWk; self.bk -= lr * self.dbk\n",
    "        self.Wv -= lr * self.dWv; self.bv -= lr * self.dbv\n",
    "        self.Wo -= lr * self.dWo; self.bo -= lr * self.dbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 03-2: Description (0 pts.)\n",
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "We keep talking about how great ReLU is, but we haven't written one yet! Lets do that now!!\n",
    "\n",
    "### Task 03-2: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Element-wise rectified linear activation.\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Apply ReLU activation and cache the input tensor.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor of any shape.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with negatives zeroed out, same shape as `X`.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Store the input and calculate and return the output of the ReLU layer\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Propagate gradients through the ReLU non-linearity.\n",
    "\n",
    "        Args:\n",
    "            dA (torch.Tensor): Upstream gradient matching the shape of the forward output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Gradient with respect to the input, zeroed where the cached input was non-positive.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate and return the gradient of the loss with respect to the input\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Keep API parity with trainable layers; ReLU has no parameters to update.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Unused learning rate argument.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Update the weights and biases of the layer using the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 03-3: Description (0 pts.)\n",
    "#### Encoder-only Transformer Model\n",
    "\n",
    "Similar to our FFN and RNN, lets make an `EncoderTransformer` class to house all of the components for our transformer model. We'll make a model with two attention blocks. GPT-3 is *just* one of these models but with 96 attention blocks (and trained on like the entirety of the internet)!\n",
    "\n",
    "### Task 03-3: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderTransformer:\n",
    "    def __init__(self, vocab_size: int, padding_idx: int, ctx_len: int = 256, d_model: int = 128):\n",
    "        self.padding_idx = padding_idx\n",
    "        self.ctx_len = ctx_len\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        # TODO: Instantiate the components of the transformer\n",
    "\n",
    "    def forward(self, X, eval=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (batch_size, seq_len) with integer token indices.\n",
    "        Returns:\n",
    "          logits: (batch_size, seq_len, vocab_size)\n",
    "          outputs: (batch_size, seq_len, hidden_size) final hidden states over time.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the output of the network\n",
    "\n",
    "        return X if not eval else self.softmax(X)\n",
    "\n",
    "    def backward(self, Y_hat, Y):\n",
    "        # TODO: Calculate the gradient of the loss with respect to the input\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Apply softmax to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor of any shape.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with softmax applied, same shape as `X`.\n",
    "        \"\"\"\n",
    "        return F.softmax(X, dim=-1)\n",
    "\n",
    "    def cross_entropy(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix of shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels of shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: The average cross-entropy loss over all m examples.\n",
    "\n",
    "        Notes:\n",
    "            A small constant epsilon is added to Y_hat to avoid computing log(0).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the cross-entropy loss\n",
    "\n",
    "    def get_accuracy(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute the classification accuracy.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix from the network, shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy as a fraction between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the accuracy of the network\n",
    "\n",
    "    def test_on_letter(self, dataset):\n",
    "        kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"\n",
    "        x = dataset.encode(kidnapping_letter).unsqueeze(0).to(self.device)  # (1, T)\n",
    "\n",
    "        preds = self.forward(x)\n",
    "        preds  = preds.argmax(dim=-1)  # (1, T)\n",
    "        predicted_deciphered = \"\".join(dataset.idx2char[int(i)] for i in preds.squeeze(0))\n",
    "\n",
    "        print(f\"\\tInput (ROT13): {kidnapping_letter}\")\n",
    "        print(f\"\\tPredicted original: {predicted_deciphered}\")\n",
    "\n",
    "        return predicted_deciphered\n",
    "\n",
    "    def train(self, train_loader, epochs=100, learning_rate=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (784, m), where each column is a flattened MNIST style image.\n",
    "            Y (numpy.ndarray): One-hot encoded labels with shape (n_classes, m), where n_classes is 26\n",
    "            epochs (int): Number of epochs for training.\n",
    "            learning_rate (float): Learning rate for the parameter updates.\n",
    "            batch_size (int, optional): Number of examples per mini-batch. Default is 32.\n",
    "            verbose (bool, optional): If True, prints training progress every 500 epochs. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'loss_history': List of loss values for each epoch.\n",
    "                - 'accuracy_history': List of accuracy values for each epoch.\n",
    "\n",
    "        Process:\n",
    "            - Shuffles the dataset each epoch.\n",
    "            - Processes data in mini-batches.\n",
    "            - Performs a forward pass, backpropagation, and parameter updates for each mini-batch.\n",
    "            - Computes the loss and accuracy for the entire dataset after each epoch.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                # Forward propagation\n",
    "                # TODO: Calculate the output of the network\n",
    "\n",
    "                # Calculate metrics for the whole epoch\n",
    "                loss = self.cross_entropy(Y_hat_batch, Y_batch)\n",
    "                accuracy = self.get_accuracy(Y_hat_batch, Y_batch)\n",
    "                \n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy.item())\n",
    "                \n",
    "                # Backward propagation\n",
    "                # TODO: Calculate the gradients of the loss with respect to the input\n",
    "                \n",
    "                # Update parameters\n",
    "                # TODO: Update the weights and biases of the layer using the learning rate\n",
    "\n",
    "            loss_history.append(np.mean(batch_losses))\n",
    "            accuracy_history.append(np.mean(batch_accuracies))\n",
    "            \n",
    "            if verbose and i % (epochs // 4) == 0:\n",
    "                print(f\"Epoch {i+1}/{epochs}\")\n",
    "                print(f\"loss: {loss_history[-1]:.5f}\")\n",
    "                print(f\"accuracy: {accuracy_history[-1]:.5f}\")\n",
    "                print('Output test:')\n",
    "                self.test_on_letter(train_loader.dataset)\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        return {'loss_history': loss_history, 'accuracy_history': accuracy_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 03-4: Description (0 pts.)\n",
    "#### EncoderTransformer Training and Testing\n",
    "\n",
    "Now lets actually train and test our model!\n",
    "\n",
    "### Task 03-4: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = dataset.vocab_size\n",
    "padding_idx = dataset.padding_idx\n",
    "\n",
    "# TODO: Instantiate the model, train it, and test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 03-4: Reference Output (0 pts.)\n",
    "\n",
    "Approximate Runtime: `6m 9.2s`\n",
    "\n",
    "```\n",
    "Epoch 1/100\n",
    "loss: 12.05548\n",
    "accuracy: 0.04005\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: b<UNK>zi<PAD>ua<UNK>a<UNK>zauzbel<UNK>az<UNK>beuboue zioa<UNK>hb<UNK>beu<UNK>uobhbu<UNK>beu<UNK>lho<UNK><UNK>ho<UNK>beu<UNK>suz u<UNK>b<UNK>a<UNK>zauz<UNK><UNK>uhsie<UNK>ba<UNK>o<UNK>zu<UNK>bzz<UNK>lub<UNK>h<UNK>ho<UNK><UNK>bbe<UNK>bb<UNK>ho<UNK><UNK>uzz<UNK>eisu <UNK>zzo<UNK>eiziao<UNK> bl<UNK>zuo<UNK>i<UNK>b<UNK>beu<UNK><UNK>iazbehbbie<UNK>i <UNK>beu<UNK>shazi<UNK>g<UNK>hz<UNK>au<UNK>ie<UNK>zi<UNK>guz<UNK>ie<UNK>beu<UNK>ou<UNK>iea<UNK> ziiz<UNK>i <UNK> bb<SOS><UNK>ibeuz<UNK>bou<UNK>b<UNK>ha<UNK>be<UNK>zuhz<UNK>bzi<UNK>zzu\n",
    "------------------------------\n",
    "Epoch 26/100\n",
    "loss: 0.57496\n",
    "accuracy: 0.94361\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i lo<PAD>ed murderinz mr theisenaloyd at the estate the zas xas the peraect murder xeapon im sure ill zet axay xith it as xell hopeaully nobody aizures out the combination oa the padlocg abcde on locger on the second aloor oa ait<SOS> otherxise i am in real trouble\n",
    "------------------------------\n",
    "Epoch 51/100\n",
    "loss: 0.20144\n",
    "accuracy: 0.96861\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenyloyd at the estate the gas xas the peryect murder xeapon im sure ill get axay xith it as xell hopeyully nobody yigures out the combination oy the padlocg abcde on locger on the second yloor oy yit<SOS> otherxise i am in real trouble\n",
    "------------------------------\n",
    "Epoch 76/100\n",
    "loss: 0.10249\n",
    "accuracy: 0.98987\n",
    "Output test:\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock abcde on locker on the second floor of fit<SOS> otherwise i am in real trouble\n",
    "------------------------------\n",
    "\tInput (ROT13): v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "\tPredicted original: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock abcde on locker on the second floor of fit<SOS> otherwise i am in real trouble\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 04: Transformer Comparisons\n",
    "\n",
    "Similar to what we've been doing, lets see how our model stacks up against the actual pytorch implementat of a transformer!\n",
    "\n",
    "### Task 04-1: Description (0 pts.)\n",
    "#### Positional Embedding Class\n",
    "\n",
    "The first thing we'll need to do is create a special \"positional-encoding\" class so we can do all of that crazy sine wave stuff to modify our input vectors to reflect their location in the input text.\n",
    "\n",
    "### Task 04-1: Code (0 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- PositionalEncoding unchanged ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, E)\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 04-2: Description (0 pts.)\n",
    "#### Encoder-Only Transformer Class\n",
    "\n",
    "Now that we have our PositionalEncoding class, lets actually create the transformer class\n",
    "\n",
    "### Task 04-2: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rot13TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-only: forward(src) -> logits over vocab for each token (B, T, V).\n",
    "    Matches RNN behavior: aligned per-timestep prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                 dim_feedforward, dropout, max_len, padding_idx):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True  # <- no transposes\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_encoder_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):                 # src: (B, T) indices\n",
    "        key_padding_mask = (src == self.padding_idx)  # (B, T) bool\n",
    "        kpm = key_padding_mask if key_padding_mask.any() else None\n",
    "\n",
    "        # TODO: Implement the forward pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 04-3: Description (0 pts.)\n",
    "#### Encoder-Only Transformer Training and Testing\n",
    "\n",
    "Now we have everything we need to train and test the torch implementation of a transformer, so lets do that and see how it compares to our hand-made one!\n",
    "\n",
    "### Task 04-3: Code (1 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "          else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "model = Rot13TransformerEncoder(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    max_len=max_len,\n",
    "    padding_idx=dataset.padding_idx\n",
    ").to(device)\n",
    "\n",
    "# TODO: Instantiate the optimizer and loss function\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total = 0.0\n",
    "        for src, tgt_output in dataloader:\n",
    "            if tgt_output.dim() == 3:\n",
    "                tgt_output = tgt_output.argmax(dim=-1)   # (B,T)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # TODO: Call the forward pass\n",
    "\n",
    "            B, T, V = logits.shape\n",
    "\n",
    "            loss = criterion(logits.reshape(B*T, V), tgt_output.reshape(B*T))\n",
    "\n",
    "            # TODO: Backpropagate and Gradient Descent\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total/len(dataloader):.4f}\")\n",
    "\n",
    "train(model, train_loader, optimizer, criterion, num_epochs=25)\n",
    "\n",
    "kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode string to indices (1, T)\n",
    "    src = dataset.encode(kidnapping_letter).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run through model\n",
    "    logits = model(src)               # (1, T, V)\n",
    "    preds = logits.argmax(dim=-1)     # (1, T)\n",
    "\n",
    "    # Decode predicted indices back to string\n",
    "    output_str = dataset.decode(preds.squeeze(0))\n",
    "\n",
    "print(f\"Input:  {kidnapping_letter}\")\n",
    "print(f\"Output: {output_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 04-3: Reference Output (0 pts.)\n",
    "\n",
    "Approximate Runtime: `3m 2.3s`\n",
    "\n",
    "```\n",
    "Epoch 1, Loss: 0.1295\n",
    "Epoch 2, Loss: 0.0029\n",
    "Epoch 3, Loss: 0.0013\n",
    "Epoch 4, Loss: 0.0008\n",
    "Epoch 5, Loss: 0.0005\n",
    "Epoch 6, Loss: 0.0004\n",
    "Epoch 7, Loss: 0.0003\n",
    "Epoch 8, Loss: 0.0002\n",
    "Epoch 9, Loss: 0.0002\n",
    "Epoch 10, Loss: 0.0001\n",
    "Epoch 11, Loss: 0.0001\n",
    "Epoch 12, Loss: 0.0001\n",
    "Epoch 13, Loss: 0.0001\n",
    "Epoch 14, Loss: 0.0001\n",
    "Epoch 15, Loss: 0.0001\n",
    "Epoch 16, Loss: 0.0000\n",
    "Epoch 17, Loss: 0.0000\n",
    "Epoch 18, Loss: 0.0000\n",
    "Epoch 19, Loss: 0.0000\n",
    "Epoch 20, Loss: 0.0000\n",
    "Epoch 21, Loss: 0.0000\n",
    "Epoch 22, Loss: 0.0000\n",
    "Epoch 23, Loss: 0.0000\n",
    "Epoch 24, Loss: 0.0000\n",
    "Epoch 25, Loss: 0.0000\n",
    "Input:  v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx 69 ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\n",
    "Output: i loved murdering mr theisenfloyd at the estate the gas was the perfect murder weapon im sure ill get away with it as well hopefully nobody figures out the combination of the padlock ee on locker on the second floor of fitz otherwise i am in real trouble\n",
    "```\n",
    "\n",
    "### Task 04-4: Transformer Short Answer Questions (2 pts.)\n",
    "\n",
    "* What are some benefits of transformers compared to RNNs?\n",
    "    * [ANSWER]\n",
    "\n",
    "* What do Q, K, and V stand for and what do they represent?\n",
    "    * [ANSWER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 05: Final Evidence Collection\n",
    "### Task 05-1: Selfie with Evidence (1 pt.)\n",
    "#### Actually photographing the evidence\n",
    "\n",
    "Now that you have the code for the locker, hopefully you have enough to put this villain behind bars! Head over to the locker and open it, take a picture of the evidence and yourselves and submit it along with your code, the TAs will handle the police report! Remember, this is an active investigation and tampering with the evidence will result in a felony charge!\n",
    "\n",
    "**NOTE:** You can just submit your photo as a file on github alongside your notebook. All group-members need to be in the photo for credit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
